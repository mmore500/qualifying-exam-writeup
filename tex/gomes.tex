\section{Cooperative coevolution of morphologically heterogeneous robots \cite{gomes2015cooperative}}

Gomes et al. devise a scenario where two robotic agents with different controls and capabilities --- a drone and a ground robot --- cooperate in order to collect items scattered over an unbounded two-dimensional space as quickly as possible.
The fitnesses of the drone and the ground robot are taken as the number of items collected.
The drone can detect objects over a wider surface area, but has no mechanism to actually retrieve them.
The ground robot, in contrast, can pick objects up but can only sense very near objects.
The ground robot and the drone use upwards- and downwards-facing sensors, respectively, to monitor each other's position.
These sensors have a limited range and field of view; if the ground robot and drone move too far apart in the arena or the drone flies at excessive altitude, the sensors of each will lose track of the other.
No communication between agents occurs besides each sensing the other's relative position.
Both robots are controlled by simple evolving neural controllers using NEAT \cite{stanley2002evolving}.
In addition to the inputs and outputs described, the drone was able to sense and, in some trials, control its own altitude.

For item placement, the rectangular arena is divided into six distinct rectangular sub-quadrants.
Then, a single item is deposited at a randomly drawn location within each of the six rectangular sub-quadrant.
This ensures that the items are somewhat evenly distributed while leaving the exact positions of the objects variable.

Gomes et al. use a cooperative co-evolution approach to simultaneously evolve controllers for the drone and the ground robot.
Separate populations of drone controllers and ground robot controllers are initiated.
For the first generation, each ground robot controller is evaluated with a random drone controller and vice versa.
Then, for subsequent generations, each ground robot controller is evaluated with the best-performing drone controller from the previous generation and each drone controller is evaluated with the best-performing ground robot controller from the previous generation.
This cooperate co-evolution method helps to detangle the credit assignment problem by ensuring that the fitness gradient (i.e., how fitness differs between agents) depends primarily on agents' own actions.
Intuitively, because each agent has the same best-available partner during its evaluation, in the case where agent A's team only collects two items but agent B's team collects five items the difference between the teams responsible for the performance discrepancy is agent A versus agent B.
Holding the other elements of the multi-agent system constant while evaluating agents from a single population helps us isolate credit --- or blame --- to the agents we're evaluating.

The cooperative co-evolution approach succeeds consistently in a baseline treatment (``Fix-Tog'') where the ground robot and drone are start at the same location and the drone's altitude is fixed (optimally for sensing).
In this treatment, nascent cooperation between the the drone and ground robot can take hold essentially immediately at the beginning in the evolutionary process and, subsequently, be refined.

Three other treatments are studied:
\begin{enumerate}
\item the drone's altitude is fixed and the drone and ground robot begin separated out of sensor range (``Fix-Sep''),
\item the drone is allowed to control its own altitude (it can lose contact with the robot by flying too high or flying too low, leaving the robot's field of view) and the ground robot and drone start at the same location (``Var-Tog''), and
\item the drone controls altitude and the ground robot and drone begin separated out of sensor range (``Var-Sep'').
\end{enumerate}
Each variable represents a potential hurdle that must be cleared before meaningful cooperation can take hold.
If the drone and robot start out of range, they must evolve to locate each other before they can begin cooperating.
If the drone can control its own altitude, it must evolve to regulate its own altitude (and fly at an appropriate altitude for cooperation).
Under each of these treatments, fully successful behaviors (i.e., the team collects all six items in the allocated time) evolve in some, but not most, evolutionary runs.


Two loss of fitness gradients \cite{wiegand2003analysis} and convergence to mediocre stable states \cite{panait2010theoretical}.

Four tr
Cooperative co-evolution succeeds when the
